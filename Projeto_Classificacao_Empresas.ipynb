{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxMitsuya/classificacao_empresas/blob/main/Projeto_Classificacao_Empresas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Categorização de todas as empresas do Brasil para classificação de Good Places to work\n",
        "\n",
        "Este é um projeto de ETL (Extract, Transform and Load) que visa realizar a coleta dos dados de todas as empresas do Brasil no site da receita federal, extrair os dados em formato .csv, realizar o tratamento desses dados e classificar segundo seu cnae fiscal."
      ],
      "metadata": {
        "id": "B5WwV62aNF_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coleta e Extração dos dados"
      ],
      "metadata": {
        "id": "s9UKaW5tNcbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manter a conexão do google drive ativa\n",
        "from google.colab import output\n",
        "output.eval_js('google.colab.kernel.proxyPort(5000)')"
      ],
      "metadata": {
        "id": "Do2ITwfwNgC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importe das bibliotecas\n",
        "import os #Manipulação dos arquivos\n",
        "import requests #Requisição de páginas web\n",
        "from bs4 import BeautifulSoup #WebScraping\n",
        "from urllib.parse import urljoin #Join de urls\n",
        "from concurrent.futures import ThreadPoolExecutor #Paralelismo de código\n",
        "from google.colab import drive #Ativar o drive"
      ],
      "metadata": {
        "id": "85kJ3CHaNg0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_dados(url_base, data_coleta):\n",
        "\n",
        "  # URL da página que contém os arquivos\n",
        "  url = urljoin(url_base, data_coleta)\n",
        "\n",
        "  # Diretório no Google Drive onde os arquivos serão salvos\n",
        "  save_directory = \"/content/drive/My Drive/\" + data_coleta\n",
        "\n",
        "  # Cria o diretório se ele não existir\n",
        "  if not os.path.exists(save_directory):\n",
        "      os.makedirs(save_directory)\n",
        "\n",
        "  # Função para baixar um arquivo\n",
        "  def download_file(file_url, file_name):\n",
        "      print(f\"Baixando {file_name}...\")\n",
        "      try:\n",
        "          with requests.get(file_url, stream=True) as r:\n",
        "              r.raise_for_status()\n",
        "              with open(file_name, 'wb') as f:\n",
        "                  for chunk in r.iter_content(chunk_size=8192):\n",
        "                      f.write(chunk)\n",
        "          print(f\"{file_name} baixado com sucesso!\")\n",
        "      except Exception as e:\n",
        "          print(f\"Erro ao baixar {file_name}: {e}\")\n",
        "\n",
        "  # Faz a requisição para a página\n",
        "  response = requests.get(url)\n",
        "  response.raise_for_status()  # Verifica se a requisição foi bem-sucedida\n",
        "\n",
        "  # Parseia o conteúdo HTML da página\n",
        "  soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "  # Lista para armazenar as URLs dos arquivos\n",
        "  file_urls = []\n",
        "\n",
        "  # Encontra todos os links na página\n",
        "  for link in soup.find_all('a'):\n",
        "      href = link.get('href')\n",
        "      if href and href.endswith('.zip'):\n",
        "          file_url = urljoin(url, href)\n",
        "          file_name = os.path.join(save_directory, href.split('/')[-1])\n",
        "          file_urls.append((file_url, file_name))\n",
        "\n",
        "  # Baixa os arquivos em paralelo usando ThreadPoolExecutor\n",
        "  with ThreadPoolExecutor(max_workers=5) as executor:  # Ajuste o número de workers conforme necessário\n",
        "      executor.map(lambda args: download_file(*args), file_urls)\n",
        "\n",
        "  print(\"Todos os arquivos foram baixados.\")"
      ],
      "metadata": {
        "id": "skvqrJwRNkNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obter a data do arquivo mais atual\n",
        "from datetime import datetime, timedelta\n",
        "def data(hoje):\n",
        "  hoje = hoje\n",
        "  mes_passado = hoje.replace(day=1) - timedelta(days=1)\n",
        "  mes = mes_passado.month\n",
        "  ano = mes_passado.year\n",
        "  mes_str = '0' + str(mes) if mes < 10 else str(mes)\n",
        "  data_coleta = f'{ano}' + '-' +mes_str + '/'\n",
        "  return data_coleta\n",
        "\n",
        "hoje = datetime.now()\n",
        "data_coleta = data(hoje)"
      ],
      "metadata": {
        "id": "mtoR-a4uNqbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url_base = \"https://arquivos.receitafederal.gov.br/dados/cnpj/dados_abertos_cnpj/\"\n",
        "\n",
        "#Baixa os dados usando a função download_dados()\n",
        "download_dados(url_base, data_coleta)"
      ],
      "metadata": {
        "id": "1ELuQCp0NrVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extração dos dados"
      ],
      "metadata": {
        "id": "8MWLivvQNvgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import das bibliotecas\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "wngNpca7OACo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pastas_zipadas = \"/content/drive/MyDrive/\" + data_coleta #Local da pasta com os arquivos zipados\n",
        "pasta_extraida = pastas_zipadas + data_coleta #Local onde os arquivos vão ser descompactados"
      ],
      "metadata": {
        "id": "EaGxvI_qN2_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(pasta_extraida, exist_ok=True) #Verifica se a pasta existe"
      ],
      "metadata": {
        "id": "GsAhMwz_N5aK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cria uma lista com todos os arquivos zip dentro da pasta\n",
        "arquivos_zip = [f for f in os.listdir(pastas_zipadas) if f.endswith(\".zip\")]"
      ],
      "metadata": {
        "id": "gsovbGz3N9EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Para cada arquivo dentro da pasta zipada, extrai o conteúdo e renomeia o arquivo com o nome do arquivo zip.\n",
        "#Exemplo: arquivo zip = cnae.zip, arquivo extraído = F.K03200$Z.D50111.CNAECSV, arquivo renomeado = cnae.csv\n",
        "for arquivo in arquivos_zip:\n",
        "    zip_path = os.path.join(pastas_zipadas, arquivo)\n",
        "    nome_zip = os.path.splitext(arquivo)[0]\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        arquivos_extraidos = zip_ref.namelist()\n",
        "        zip_ref.extractall(pasta_extraida)  # Extrai todos os arquivos na pasta destino\n",
        "\n",
        "        arquivo_original = os.path.join(pasta_extraida, arquivos_extraidos[0])  # Primeiro arquivo extraído\n",
        "        novo_nome = os.path.join(pasta_extraida, f\"{nome_zip}.csv\")  # Novo nome do arquivo\n",
        "\n",
        "        os.rename(arquivo_original, novo_nome)  # Renomeia o arquivo\n",
        "        print(f\"✔ {arquivo} extraído e renomeado para {nome_zip}.csv\")"
      ],
      "metadata": {
        "id": "XOQtU937Nz3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "99gXLvj6OIq8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importações de bibliotecas necessárias\n",
        "\n",
        "Este notebook realiza o processamento de dados do CNPJ brasileiro, convertendo arquivos CSV em Parquet e aplicando diversas transformações para facilitar análises posteriores.\n",
        "Bibliotecas principais: PySpark para processamento distribuído e módulos do sistema para manipulação de arquivos."
      ],
      "metadata": {
        "id": "1ncStHNk7m1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
        "from pyspark.sql.functions import col, concat, lit, broadcast, regexp_replace, substring\n",
        "from pyspark.sql.functions import when, to_date, date_format, create_map, count, isnan\n",
        "from pyspark.sql.functions import explode, split, collect_list\n",
        "from itertools import chain\n",
        "import time\n",
        "import gc"
      ],
      "metadata": {
        "id": "nuiRcfnE9f7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta seção configura o ambiente de trabalho, definindo os diretórios necessários para armazenar os arquivos originais e processados. O Google Drive é montado para facilitar o acesso persistente aos dados entre sessões."
      ],
      "metadata": {
        "id": "jLKgwUEm79k6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============== CONFIGURAÇÕES E INICIALIZAÇÃO ===============\n",
        "\n",
        "# Montar o Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Definir a pasta compartilhada onde os arquivos CSV estão localizados\n",
        "pasta_compartilhada = '/content/drive/MyDrive/arquivos_cnpj/2025-01'\n",
        "\n",
        "# Definir pasta para salvar arquivos Parquet\n",
        "pasta_parquet = os.path.join(pasta_compartilhada, 'parquet')\n",
        "\n",
        "# Criar diretório para arquivos Parquet se não existir\n",
        "if not os.path.exists(pasta_parquet):\n",
        "    os.makedirs(pasta_parquet)\n",
        "\n",
        "# Mudar para o diretório de trabalho\n",
        "os.chdir(pasta_compartilhada)"
      ],
      "metadata": {
        "id": "iltQyBdc-nUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c5d3fe8-d113-4294-fd0a-a0237dae0762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuração otimizada da sessão Spark para lidar com grandes volumes de dados\n",
        "\n",
        "Parâmetros foram ajustados para equilibrar o uso de memória, paralelismo e performance.\n",
        "Os parâmetros abaixo foram especificamente configurados para:\n",
        "- Melhorar a distribuição da carga de trabalho entre executores;\n",
        "- Otimizar operações de join e shuffle que são intensivas em recursos;\n",
        "- Habilitar otimizações adaptativas do Spark para melhor desempenho em consultas complexas."
      ],
      "metadata": {
        "id": "HKwAzUbW8H7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============== OTIMIZAÇÃO DA SESSÃO SPARK ===============\n",
        "\n",
        "# Criar uma sessão Spark com configurações otimizadas\n",
        "spark = SparkSession.builder \\\n",
        "    .appName('Análise de Dados CNPJ') \\\n",
        "    .config(\"spark.driver.memory\", \"16g\") \\\n",
        "    .config(\"spark.executor.memory\", \"8g\") \\\n",
        "    .config(\"spark.executor.cores\", \"4\") \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "    .config(\"spark.default.parallelism\", \"100\") \\\n",
        "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
        "    .config(\"spark.memory.storageFraction\", \"0.2\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .config(\"spark.kryoserializer.buffer.max\", \"1g\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.files.maxPartitionBytes\", \"134217728\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Log de configurações\n",
        "print(\"Configurações Spark:\")\n",
        "for conf in sorted(spark.sparkContext.getConf().getAll()):\n",
        "    print(conf)"
      ],
      "metadata": {
        "id": "pmrsU4JO-fQE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a53265d8-9d76-4e67-c2a1-cfdd51fa7eb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configurações Spark:\n",
            "('spark.app.id', 'local-1742999352474')\n",
            "('spark.app.name', 'Análise de Dados CNPJ')\n",
            "('spark.app.startTime', '1742999352344')\n",
            "('spark.app.submitTime', '1742993255832')\n",
            "('spark.default.parallelism', '100')\n",
            "('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n",
            "('spark.driver.host', '113c4c15290f')\n",
            "('spark.driver.maxResultSize', '4g')\n",
            "('spark.driver.memory', '16g')\n",
            "('spark.driver.port', '43305')\n",
            "('spark.executor.cores', '4')\n",
            "('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n",
            "('spark.executor.id', 'driver')\n",
            "('spark.executor.memory', '8g')\n",
            "('spark.kryoserializer.buffer.max', '1g')\n",
            "('spark.master', 'local[*]')\n",
            "('spark.memory.fraction', '0.8')\n",
            "('spark.memory.storageFraction', '0.2')\n",
            "('spark.rdd.compress', 'True')\n",
            "('spark.serializer', 'org.apache.spark.serializer.KryoSerializer')\n",
            "('spark.serializer.objectStreamReset', '100')\n",
            "('spark.sql.adaptive.enabled', 'true')\n",
            "('spark.sql.adaptive.skewJoin.enabled', 'true')\n",
            "('spark.sql.files.maxPartitionBytes', '134217728')\n",
            "('spark.sql.shuffle.partitions', '200')\n",
            "('spark.sql.warehouse.dir', 'file:/content/drive/MyDrive/arquivos_cnpj/2025-01/spark-warehouse')\n",
            "('spark.submit.deployMode', 'client')\n",
            "('spark.submit.pyFiles', '')\n",
            "('spark.ui.showConsoleProgress', 'true')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definição dos cabeçalhos e estruturas para cada tipo de arquivo\n",
        "\n",
        "Os arquivos CSV da base de dados do CNPJ seguem uma estrutura específica definida pela Receita Federal.\n",
        "Esta seção mapeia cada coluna para seu respectivo nome, facilitando o entendimento dos dados e, também define o mapeamento de categorias CNAE para classificação setorial das empresas."
      ],
      "metadata": {
        "id": "YUmiBK768iaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============== DEFINIÇÃO DE CABEÇALHOS E SCHEMAS ===============\n",
        "\n",
        "# Definir os cabeçalhos dos arquivos CSV para os DataFrames\n",
        "cnae_header = ['ID_Cnae', 'Cnae']\n",
        "motivo_header = ['ID_Motivo', 'Motivo']\n",
        "municipios_header = ['ID_Municipio', 'Municipio']\n",
        "natureza_header = ['ID_Natureza', 'Natureza']\n",
        "paises_header = ['ID_Pais', 'Pais']\n",
        "qualificacoes_header = ['ID_Qualificacao', 'Qualificacao']\n",
        "simples_header = ['CNPJ_Basico', 'Opcao_pelo_Simples', 'Data_de_Opcao_pelo_Simples',\n",
        "                 'Data_de_Exclusao_do_Simples', 'Opcao_pelo_MEI', 'Data_de_Opcao_pelo_MEI',\n",
        "                 'Data_de_Exclusao_do_MEI']\n",
        "empresas_header = ['CNPJ_Basico', 'Razao_Social_Nome_Empresarial', 'Codigo_Natureza_Juridica',\n",
        "                  'Qualificacao_do_Responsavel', 'Capital_Social_da_Empresa', 'Porte_da_Empresa',\n",
        "                  'Ente_Federativo_Responsavel']\n",
        "estabelecimentos_header = ['CNPJ_Basico', 'CNPJ_Ordem', 'CNPJ_Dv', 'Identificador_Matriz_Filial',\n",
        "                          'Nome_Fantasia', 'Situacao_Cadastral', 'Data_Situacao_Cadastral',\n",
        "                          'Motivo_Situacao_Cadastral', 'Nome_da_Cidade_no_Exterior', 'Pais',\n",
        "                          'Data_de_Inicio_Atividade', 'Cnae_Fiscal_Principal', 'Cnae_Fiscal_Secundaria',\n",
        "                          'Tipo_de_Logradouro', 'Logradouro', 'Numero', 'Complemento', 'Bairro', 'Cep',\n",
        "                          'UF', 'Municipio', 'DDD_1', 'Telefone_1', 'DDD_2', 'Telefone_2', 'DDD_do_Fax',\n",
        "                          'Fax', 'Correio_Eletronico', 'Situacao_Especial', 'Data_da_Situacao_Especial']\n",
        "socios_header = ['CNPJ_Basico', 'Identitificador_de_Socio', 'Nome_do_Socio_ou_Razao_Social',\n",
        "                'CNPJ_ou_CPF_do_Socio', 'Qualificacao_do_Socio', 'Data_de_Entrada_na_Sociedade',\n",
        "                'Pais_Socio', 'Representante_Legal', 'Nome_do_Representante',\n",
        "                'Qualificacao_do_Representante_Legal', 'Faixa_Etaria']\n",
        "\n",
        "# Colunas com datas\n",
        "date_simples = ['Data_de_Opcao_pelo_Simples', 'Data_de_Exclusao_do_Simples',\n",
        "               'Data_de_Opcao_pelo_MEI', 'Data_de_Exclusao_do_MEI']\n",
        "date_estabelecimentos = ['Data_Situacao_Cadastral', 'Data_de_Inicio_Atividade',\n",
        "                        'Data_da_Situacao_Especial']\n",
        "date_socios = ['Data_de_Entrada_na_Sociedade']\n",
        "\n",
        "# Dicionário de categorias Cnae\n",
        "cnae_map = {\n",
        "    '6311': 'Tecnologia e Inovação (TI e Software)',\n",
        "    '6312': 'Tecnologia e Inovação (TI e Software)',\n",
        "    '7810': 'Plataformas de emprego e recrutamento',\n",
        "    '6319': 'Plataformas de emprego e recrutamento',\n",
        "    '6422': 'Bancos',\n",
        "    '6423': 'Bancos',\n",
        "    '6424': 'Bancos',\n",
        "    '6431': 'Financeiras',\n",
        "    '6432': 'Financeiras',\n",
        "    '6433': 'Financeiras',\n",
        "    '6440': 'Financeiras',\n",
        "    '4789': 'Conteúdo sensível (sex shop e acompanhantes)',\n",
        "    '9609': 'Conteúdo sensível (sex shop e acompanhantes)',\n",
        "    '4771': 'Moda',\n",
        "    '7410': 'Moda',\n",
        "    '4776': 'Pet',\n",
        "    '7500': 'Pet',\n",
        "    '5111': 'Transporte',\n",
        "    '5112': 'Transporte',\n",
        "    '5229': 'Logística e Armazenagem',\n",
        "    '5320': 'Logística e Armazenagem',\n",
        "    '7020': 'Consultoria e Treinamento Profissional',\n",
        "    '8599': 'Consultoria e Treinamento Profissional',\n",
        "    '62': 'Tecnologia e Inovação (TI e Software)',\n",
        "    '73': 'Agências de Marketing e Publicidade',\n",
        "    '68': 'Setor imobiliário',\n",
        "    '92': 'Apostas',\n",
        "    '96': 'Beleza e Estética',\n",
        "    '85': 'Educação',\n",
        "    '65': 'Seguradoras',\n",
        "    '14': 'Moda',\n",
        "    '47': 'Comércio e varejo',\n",
        "    '86': 'Saúde',\n",
        "    '79': 'Turismo e Lazer',\n",
        "    '93': 'Turismo e Lazer',\n",
        "    '51': 'Empresas aéreas',\n",
        "    '41': 'Construção e Infraestrutura',\n",
        "    '01': 'Agronegócio e Pecuária',\n",
        "    '82': 'Serviços Administrativos e de Apoio',\n",
        "    '49': 'Transporte',\n",
        "    '50': 'Transporte',\n",
        "    '29': 'Veículos',\n",
        "    '45': 'Veículos',\n",
        "    '10': 'Alimentos e Bebidas',\n",
        "    '11': 'Alimentos e Bebidas'\n",
        "}"
      ],
      "metadata": {
        "id": "fnKengRy-a1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conjunto de funções auxiliares para as operações de ETL (Extração, Transformação e Carga)\n",
        "\n",
        "Estas funções encapsulam operações complexas e repetitivas do processo, incluindo:\n",
        "- Monitoramento de memória para evitar problemas de OOM (Out of Memory);\n",
        "- Definição de schemas para leitura estruturada dos CSVs;\n",
        "- Conversão e persistência dos dados em formato Parquet (mais eficiente para análises);\n",
        "- Operações de join otimizadas para grandes volumes de dados;\n",
        "- Transformações de tipos de dados (datas, valores monetários, etc.);\n",
        "- Funções para análise de qualidade dos dados (nulos, duplicatas)."
      ],
      "metadata": {
        "id": "A6Up459h81OE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============== FUNÇÕES AUXILIARES ===============\n",
        "\n",
        "def log_memory_usage():\n",
        "    \"\"\"Registra o uso de memória atual.\"\"\"\n",
        "    from psutil import virtual_memory\n",
        "    mem = virtual_memory()\n",
        "    print(f\"Memória: {mem.percent}% usada ({mem.used / 1024 / 1024 / 1024:.2f} GB de {mem.total / 1024 / 1024 / 1024:.2f} GB)\")\n",
        "\n",
        "def Schema(header, integers):\n",
        "    \"\"\"\n",
        "    Cria um schema para leitura de CSV.\n",
        "    Args:\n",
        "        header (list): Lista de nomes das colunas.\n",
        "        integers (list): Colunas que devem ser inteiras.\n",
        "    Returns:\n",
        "        StructType: Schema do DataFrame.\n",
        "    \"\"\"\n",
        "    schema = StructType([\n",
        "        StructField(col, StringType()) if col not in integers else StructField(col, IntegerType())\n",
        "        for col in header\n",
        "    ])\n",
        "    return schema\n",
        "\n",
        "def Carregar_Dados_Parquet(csv_path, schema, header, parquet_path):\n",
        "    \"\"\"\n",
        "    Carrega dados de CSV, aplica schema e salva em formato Parquet.\n",
        "\n",
        "    Args:\n",
        "        csv_path: Nome do arquivo CSV\n",
        "        schema: Schema do DataFrame\n",
        "        header: Lista com nomes das colunas\n",
        "        parquet_path: Caminho para salvar o arquivo Parquet\n",
        "\n",
        "    Returns:\n",
        "        DataFrame carregado do Parquet\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Verifica se o arquivo Parquet já existe\n",
        "        if not os.path.exists(parquet_path):\n",
        "            print(f\"Convertendo {csv_path} para Parquet...\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Lê o CSV com o schema definido\n",
        "            df = spark.read.csv(\n",
        "                csv_path,\n",
        "                header=False,\n",
        "                sep=';',\n",
        "                schema=schema,\n",
        "                encoding='latin1'\n",
        "            ).toDF(*header)\n",
        "\n",
        "            # Salva em formato Parquet com compressão Snappy (bom equilíbrio entre compressão e velocidade)\n",
        "            df.write.option(\"compression\", \"snappy\").parquet(parquet_path)\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "            print(f\"Conversão concluída em {elapsed:.2f} segundos\")\n",
        "\n",
        "        # Carrega do Parquet\n",
        "        return spark.read.parquet(parquet_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao processar {csv_path}: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def Carregar_Dados_Multiplos_Parquet(csv_paths, schema, header, parquet_base_path):\n",
        "    \"\"\"\n",
        "    Carrega múltiplos arquivos CSV, aplica schema e salva em formato Parquet.\n",
        "\n",
        "    Args:\n",
        "        csv_paths: Lista de nomes de arquivos CSV\n",
        "        schema: Schema do DataFrame\n",
        "        header: Lista com nomes das colunas\n",
        "        parquet_base_path: Diretório base para salvar os arquivos Parquet\n",
        "\n",
        "    Returns:\n",
        "        DataFrame unificado carregado do Parquet\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Verifica se o diretório Parquet existe\n",
        "        if not os.path.exists(parquet_base_path):\n",
        "            os.makedirs(parquet_base_path)\n",
        "\n",
        "            # Processa cada arquivo CSV\n",
        "            for i, csv_path in enumerate(csv_paths):\n",
        "                if not os.path.exists(csv_path):\n",
        "                    print(f\"Arquivo {csv_path} não encontrado. Pulando...\")\n",
        "                    continue\n",
        "\n",
        "                parquet_path = f\"{parquet_base_path}/parte_{i}.parquet\"\n",
        "\n",
        "                print(f\"Convertendo {csv_path} para Parquet ({i+1}/{len(csv_paths)})...\")\n",
        "                start_time = time.time()\n",
        "\n",
        "                try:\n",
        "                    # Lê o CSV com o schema definido\n",
        "                    df = spark.read.csv(\n",
        "                        csv_path,\n",
        "                        header=False,\n",
        "                        sep=';',\n",
        "                        schema=schema,\n",
        "                        encoding='latin1'\n",
        "                    ).toDF(*header)\n",
        "\n",
        "                    # Reparticionamento para melhorar performance de escrita\n",
        "                    df = df.repartition(10)\n",
        "\n",
        "                    # Salva em formato Parquet\n",
        "                    df.write.option(\"compression\", \"snappy\").parquet(parquet_path)\n",
        "\n",
        "                    elapsed = time.time() - start_time\n",
        "                    print(f\"Conversão de {csv_path} concluída em {elapsed:.2f} segundos\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Erro ao processar {csv_path}: {str(e)}\")\n",
        "\n",
        "        # Carrega todos os arquivos Parquet como um único DataFrame\n",
        "        return spark.read.parquet(f\"{parquet_base_path}/*\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao processar múltiplos arquivos: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def Join_Dataframes(df_left, df_right, column_left, column_right):\n",
        "    \"\"\"\n",
        "    Realiza um left join entre dois DataFrames e remove a coluna redundante.\n",
        "    Substitui a coluna com código pela sua descrição.\n",
        "\n",
        "    Args:\n",
        "        df_left: DataFrame à esquerda (DataFrame a ser unido).\n",
        "        df_right: DataFrame à direita (DataFrame com os dados a serem unidos).\n",
        "        column_left: Nome da coluna no DataFrame à esquerda para realizar o join.\n",
        "        column_right: Nome da coluna no DataFrame à direita para realizar o join.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame resultante do left join com a coluna redundante removida.\n",
        "    \"\"\"\n",
        "    # Conta o número de registros no DataFrame esquerdo para detecção de possíveis problemas\n",
        "    count_before = df_left.count()\n",
        "\n",
        "    # Renomeia temporariamente a coluna para evitar ambiguidade\n",
        "    column_left_temp = column_left + '_'\n",
        "    df_left = df_left.withColumnRenamed(column_left, column_left_temp)\n",
        "\n",
        "    # Verificar se df_right é pequeno o suficiente para broadcast\n",
        "    right_count = df_right.count()\n",
        "    if right_count < 10000:  # Limite arbitrário, ajuste conforme necessário\n",
        "        df_right_broadcast = broadcast(df_right)\n",
        "    else:\n",
        "        df_right_broadcast = df_right\n",
        "        print(f\"Tabela secundária muito grande para broadcast: {right_count} registros\")\n",
        "\n",
        "    # Realiza o left join\n",
        "    df = df_left.join(df_right_broadcast, df_left[column_left_temp] == df_right_broadcast[column_right], 'left')\n",
        "\n",
        "    # Remove a coluna redundante\n",
        "    df = df.drop(df_right_broadcast[column_right])\n",
        "\n",
        "    # Verifica a coluna a ser usada para substituição\n",
        "    value_col = df_right.columns[1] if len(df_right.columns) > 1 else None\n",
        "\n",
        "    if value_col:\n",
        "        # Substitui os valores da coluna temporária\n",
        "        df = df.withColumn(column_left_temp, col(value_col))\n",
        "\n",
        "        # Remove a coluna original do DataFrame da direita\n",
        "        df = df.drop(value_col)\n",
        "\n",
        "    # Renomeia a coluna temporária de volta ao nome original\n",
        "    df = df.withColumnRenamed(column_left_temp, column_left)\n",
        "\n",
        "    # Verifica se o número de linhas mudou após o join (para diagnóstico)\n",
        "    count_after = df.count()\n",
        "    if count_before != count_after:\n",
        "        print(f\"AVISO: Diferença no número de registros após join em {column_left}: antes={count_before}, depois={count_after}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def Join_Dataframes_Duplicated(df_left, df_right, column_name):\n",
        "    \"\"\"\n",
        "    Realiza um left join entre dois DataFrames e remove a coluna redundante da direita.\n",
        "    Suporta junção em colunas com o mesmo nome nos dois DataFrames.\n",
        "\n",
        "    Args:\n",
        "        df_left: DataFrame à esquerda (principal).\n",
        "        df_right: DataFrame à direita (dados complementares).\n",
        "        column_name: Nome da coluna em comum usada para junção.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame resultante do left join com a coluna duplicada removida.\n",
        "    \"\"\"\n",
        "    # Conta o número de registros no DataFrame esquerdo para detecção de possíveis problemas\n",
        "    count_before = df_left.count()\n",
        "\n",
        "    # Renomeia temporariamente a coluna da direita para evitar conflito\n",
        "    temp_column_right = column_name + \"_right\"\n",
        "    df_right = df_right.withColumnRenamed(column_name, temp_column_right)\n",
        "\n",
        "    # Seleciona apenas as colunas necessárias do DataFrame direito para reduzir o volume de dados\n",
        "    cols_to_keep = [c for c in df_right.columns if c != temp_column_right]\n",
        "    df_right_reduced = df_right.select([temp_column_right] + cols_to_keep)\n",
        "\n",
        "    # Aplica broadcast apenas se o DataFrame direito for pequeno o suficiente\n",
        "    right_count = df_right_reduced.count()\n",
        "    if right_count < 10000:  # Limite arbitrário, ajuste conforme necessário\n",
        "        df_right_broadcast = broadcast(df_right_reduced)\n",
        "    else:\n",
        "        # Para DataFrames maiores, podemos reparticionar para melhorar a performance do join\n",
        "        df_right_broadcast = df_right_reduced.repartition(50, temp_column_right)\n",
        "        print(f\"Tabela secundária muito grande para broadcast: {right_count} registros. Reparticionando.\")\n",
        "\n",
        "    # Realiza o left join\n",
        "    df = df_left.join(df_right_broadcast, df_left[column_name] == df_right_broadcast[temp_column_right], 'left')\n",
        "\n",
        "    # Remove a coluna temporária após a junção\n",
        "    df = df.drop(temp_column_right)\n",
        "\n",
        "    # Verifica se o número de linhas mudou após o join (para diagnóstico)\n",
        "    try:\n",
        "        count_after = df.count()\n",
        "        if count_before != count_after:\n",
        "            print(f\"AVISO: Diferença no número de registros após join em {column_name}: antes={count_before}, depois={count_after}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Não foi possível verificar contagem após join: {str(e)}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def Time_Convert(df, date_array):\n",
        "    \"\"\"\n",
        "    Converte as colunas de data para o tipo de dado correto.\n",
        "\n",
        "    Args:\n",
        "        df: O DataFrame.\n",
        "        date_array: Uma lista com os nomes das colunas de data.\n",
        "\n",
        "    Returns:\n",
        "        O DataFrame com as colunas de data convertidas.\n",
        "    \"\"\"\n",
        "    for col_name in date_array:\n",
        "        if col_name in df.columns:\n",
        "            df = df.withColumn(col_name, to_date(col(col_name), 'yyyyMMdd'))\n",
        "            df = df.withColumn(col_name, date_format(col(col_name), 'dd-MM-yyyy'))\n",
        "    return df\n",
        "\n",
        "def contar_nulos(df, max_cols=10):\n",
        "    \"\"\"\n",
        "    Conta valores nulos em cada coluna do DataFrame, limitando o número de colunas exibidas.\n",
        "\n",
        "    Args:\n",
        "        df: O DataFrame.\n",
        "        max_cols: Número máximo de colunas a serem exibidas.\n",
        "    \"\"\"\n",
        "    # Usando .select para criar uma projeção em vez de coletar todos os dados\n",
        "    null_counts = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns])\n",
        "\n",
        "    # Convertendo para um formato mais fácil de exibir\n",
        "    null_counts_row = null_counts.collect()[0]\n",
        "    null_dict = {c: null_counts_row[c] for c in df.columns}\n",
        "\n",
        "    # Ordenando por número de nulos (decrescente)\n",
        "    sorted_nulls = sorted(null_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Exibindo os resultados\n",
        "    print(\"Contagem de valores nulos por coluna:\")\n",
        "    for col_name, null_count in sorted_nulls[:max_cols]:\n",
        "        print(f\"{col_name}: {null_count}\")\n",
        "\n",
        "    if len(sorted_nulls) > max_cols:\n",
        "        print(f\"... e mais {len(sorted_nulls) - max_cols} colunas\")\n",
        "\n",
        "def contar_duplicatas(df, keys=None):\n",
        "    \"\"\"\n",
        "    Conta registros duplicados no DataFrame com base em chaves específicas.\n",
        "\n",
        "    Args:\n",
        "        df: O DataFrame.\n",
        "        keys: Lista de colunas para verificar duplicatas. Se None, usa todas as colunas.\n",
        "    \"\"\"\n",
        "    if keys is None:\n",
        "        keys = df.columns\n",
        "\n",
        "    try:\n",
        "        # Contagem de duplicatas (mais eficiente que groupBy + filter)\n",
        "        duplicates = df.groupBy(keys).count().filter(\"count > 1\")\n",
        "        dup_count = duplicates.count()\n",
        "\n",
        "        print(f\"Encontradas {dup_count} chaves duplicadas.\")\n",
        "\n",
        "        if dup_count > 0 and dup_count <= 5:\n",
        "            # Mostrar exemplo de duplicatas\n",
        "            print(\"Exemplos de duplicatas:\")\n",
        "            duplicates.show(5, truncate=False)\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao contar duplicatas: {str(e)}\")\n",
        "\n",
        "def process_chunk(data_chunk, safe_mode=True):\n",
        "    \"\"\"\n",
        "    Processa um subconjunto de dados para evitar sobrecarga do driver.\n",
        "\n",
        "    Args:\n",
        "        data_chunk: DataFrame a ser processado\n",
        "        safe_mode: Se True, limita operações potencialmente perigosas\n",
        "\n",
        "    Returns:\n",
        "        DataFrame processado\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"Processando chunk com {data_chunk.count()} registros\")\n",
        "\n",
        "        # Cria coluna de CNPJ completo\n",
        "        data_chunk = data_chunk.withColumn(\n",
        "            \"CNPJ\",\n",
        "            concat(col(\"CNPJ_Basico\"), lit(\"/\"), col(\"CNPJ_Ordem\"), lit(\"-\"), col(\"CNPJ_Dv\"))\n",
        "        )\n",
        "\n",
        "        # Remove colunas redundantes\n",
        "        data_chunk = data_chunk.drop(\"CNPJ_Basico\", \"CNPJ_Ordem\", \"CNPJ_Dv\")\n",
        "\n",
        "        # Em modo seguro, não realiza todas as transformações\n",
        "        if not safe_mode:\n",
        "            # Transformações adicionais aqui\n",
        "            pass\n",
        "\n",
        "        return data_chunk\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao processar chunk: {str(e)}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "SQpNEtZI-TjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inicialização dos schemas para cada tipo de arquivo\n",
        "\n",
        "Define quais campos devem ser tratados como inteiros vs. strings.\n",
        "\n",
        "Estes schemas garantem que os dados sejam lidos de forma correta e consistente."
      ],
      "metadata": {
        "id": "qTrbPSw59L9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============== INICIALIZAÇÃO DE SCHEMAS ===============\n",
        "\n",
        "# Definir schemas para cada DataFrame\n",
        "schema_cnae = Schema(cnae_header, [])\n",
        "schema_motivos = Schema(motivo_header, [])\n",
        "schema_municipios = Schema(municipios_header, [])\n",
        "schema_naturezas = Schema(natureza_header, [])\n",
        "schema_paises = Schema(paises_header, [])\n",
        "schema_qualificacoes = Schema(qualificacoes_header, [])\n",
        "schema_simples = Schema(simples_header, [])\n",
        "schema_empresas = Schema(empresas_header, ['Porte_da_Empresa'])\n",
        "schema_estabelecimentos = Schema(estabelecimentos_header, ['Identificador_Matriz_Filial', 'Situacao_Cadastral'])\n",
        "schema_socios = Schema(socios_header, ['Identitificador_de_Socio', 'Faixa_Etaria'])"
      ],
      "metadata": {
        "id": "6W3zjWSZ-IeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processo de carregamento e conversão de dados\n",
        "\n",
        "Esta etapa é dividida em dois blocos principais:\n",
        "1. Carregamento de tabelas de referência (menores);\n",
        "2. Carregamento das tabelas principais de empresas, estabelecimentos e sócios;\n",
        "\n",
        "Os arquivos são convertidos de CSV para Parquet apenas na primeira execução.\n",
        "Em execuções subsequentes, os dados são lidos diretamente do formato Parquet mais eficiente, economizando tempo de processamento"
      ],
      "metadata": {
        "id": "Ns1-87bB9XLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============== CARREGAMENTO DE DADOS ===============\n",
        "\n",
        "print(\"\\n\\n======== CARREGANDO DADOS DE REFERÊNCIA ========\")\n",
        "# Carregar dados de tabelas de referência (geralmente tabelas pequenas)\n",
        "df_cnae = Carregar_Dados_Parquet('Cnaes.csv', schema_cnae, cnae_header,\n",
        "                                os.path.join(pasta_parquet, 'cnae.parquet'))\n",
        "df_motivos = Carregar_Dados_Parquet('Motivos.csv', schema_motivos, motivo_header,\n",
        "                                   os.path.join(pasta_parquet, 'motivos.parquet'))\n",
        "df_municipios = Carregar_Dados_Parquet('Municipios.csv', schema_municipios, municipios_header,\n",
        "                                      os.path.join(pasta_parquet, 'municipios.parquet'))\n",
        "df_naturezas = Carregar_Dados_Parquet('Naturezas.csv', schema_naturezas, natureza_header,\n",
        "                                     os.path.join(pasta_parquet, 'naturezas.parquet'))\n",
        "df_paises = Carregar_Dados_Parquet('Paises.csv', schema_paises, paises_header,\n",
        "                                  os.path.join(pasta_parquet, 'paises.parquet'))\n",
        "df_qualificacoes = Carregar_Dados_Parquet('Qualificacoes.csv', schema_qualificacoes, qualificacoes_header,\n",
        "                                         os.path.join(pasta_parquet, 'qualificacoes.parquet'))\n",
        "\n",
        "print(\"\\n\\n======== CARREGANDO TABELAS PRINCIPAIS ========\")\n",
        "# Tabelas principais - verificar existência de arquivos\n",
        "arquivos_empresas = [f'Empresas{i}.csv' for i in range(10) if os.path.exists(f'Empresas{i}.csv')]\n",
        "arquivos_estabelecimentos = [f'Estabelecimentos{i}.csv' for i in range(10) if os.path.exists(f'Estabelecimentos{i}.csv')]\n",
        "arquivos_socios = [f'Socios{i}.csv' for i in range(10) if os.path.exists(f'Socios{i}.csv')]\n",
        "\n",
        "# Informações sobre arquivos encontrados\n",
        "print(f\"Encontrados {len(arquivos_empresas)} arquivos de Empresas\")\n",
        "print(f\"Encontrados {len(arquivos_estabelecimentos)} arquivos de Estabelecimentos\")\n",
        "print(f\"Encontrados {len(arquivos_socios)} arquivos de Socios\")\n",
        "\n",
        "# Carregar tabela simples\n",
        "print(\"\\nCarregando dados do Simples Nacional...\")\n",
        "df_simples = Carregar_Dados_Parquet('Simples.csv', schema_simples, simples_header,\n",
        "                               os.path.join(pasta_parquet, 'simples.parquet'))\n",
        "\n",
        "# Carregar dados de Empresas, Estabelecimentos e Socios\n",
        "print(\"\\nCarregando dados de Empresas...\")\n",
        "df_empresas = Carregar_Dados_Multiplos_Parquet(arquivos_empresas, schema_empresas, empresas_header,\n",
        "                                         os.path.join(pasta_parquet, 'empresas'))\n",
        "\n",
        "print(\"\\nCarregando dados de Estabelecimentos...\")\n",
        "df_estabelecimentos = Carregar_Dados_Multiplos_Parquet(arquivos_estabelecimentos, schema_estabelecimentos,\n",
        "                                              estabelecimentos_header,\n",
        "                                              os.path.join(pasta_parquet, 'estabelecimentos'))\n",
        "\n",
        "print(\"\\nCarregando dados de Socios...\")\n",
        "df_socios = Carregar_Dados_Multiplos_Parquet(arquivos_socios, schema_socios, socios_header,\n",
        "                                       os.path.join(pasta_parquet, 'socios'))"
      ],
      "metadata": {
        "id": "o15-8M_E-E8O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ade48cb6-616a-4291-f548-ff83c239a783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "======== CARREGANDO DADOS DE REFERÊNCIA ========\n",
            "\n",
            "\n",
            "======== CARREGANDO TABELAS PRINCIPAIS ========\n",
            "Encontrados 10 arquivos de Empresas\n",
            "Encontrados 10 arquivos de Estabelecimentos\n",
            "Encontrados 10 arquivos de Socios\n",
            "\n",
            "Carregando dados do Simples Nacional...\n",
            "\n",
            "Carregando dados de Empresas...\n",
            "\n",
            "Carregando dados de Estabelecimentos...\n",
            "\n",
            "Carregando dados de Socios...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementação de amostragem controlada para validação e teste\n",
        "\n",
        "Esta etapa reduz o volume de dados processados para uma fração do conjunto original.\n",
        "\n",
        "Isso permite validar todas as transformações e joins de forma rápida antes de executar o processamento em escala completa.\n",
        "\n",
        "A fração de amostragem pode ser ajustada conforme necessário"
      ],
      "metadata": {
        "id": "k_tcBAAS9qEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============== ESTATÍSTICAS E AMOSTRAGEM ===============\n",
        "print(\"\\n\\n======== APLICANDO AMOSTRAGEM PARA VALIDAÇÃO ========\")\n",
        "\n",
        "# Parâmetro de controle para amostragem\n",
        "MODO_AMOSTRA = True  # Altere para False para processamento completo\n",
        "FRACAO_AMOSTRA = 0.05  # 5% do volume original\n",
        "\n",
        "if MODO_AMOSTRA:\n",
        "    print(f\"Aplicando amostragem de {FRACAO_AMOSTRA*100}% aos dados...\")\n",
        "    # Salvando estatísticas antes da amostragem\n",
        "    count_empresas_original = df_empresas.count()\n",
        "    count_estabelecimentos_original = df_estabelecimentos.count()\n",
        "    count_socios_original = df_socios.count()\n",
        "\n",
        "    # Aplicando amostragem\n",
        "    df_empresas = df_empresas.sample(fraction=FRACAO_AMOSTRA, seed=42)\n",
        "    df_estabelecimentos = df_estabelecimentos.sample(fraction=FRACAO_AMOSTRA, seed=42)\n",
        "    df_socios = df_socios.sample(fraction=FRACAO_AMOSTRA, seed=42)\n",
        "\n",
        "    # Persistindo os DataFrames amostrados\n",
        "    df_empresas.persist()\n",
        "    df_estabelecimentos.persist()\n",
        "    df_socios.persist()\n",
        "\n",
        "    # Estatísticas após amostragem\n",
        "    print(f\"Empresas: {count_empresas_original:,} -> {df_empresas.count():,} registros\")\n",
        "    print(f\"Estabelecimentos: {count_estabelecimentos_original:,} -> {df_estabelecimentos.count():,} registros\")\n",
        "    print(f\"Sócios: {count_socios_original:,} -> {df_socios.count():,} registros\")\n",
        "\n",
        "    # Modificar pasta de saída para indicar que são dados de amostra\n",
        "    pasta_parquet_saida = os.path.join(pasta_parquet, 'amostra')\n",
        "    if not os.path.exists(pasta_parquet_saida):\n",
        "        os.makedirs(pasta_parquet_saida)\n",
        "else:\n",
        "    print(\"Processando conjunto de dados completo...\")\n",
        "    pasta_parquet_saida = pasta_parquet\n",
        "\n",
        "# Continuar com as transformações normalmente..."
      ],
      "metadata": {
        "id": "vgr-kbDZImWp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b41be4d-26a9-43aa-c02a-f2874f796555"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "======== APLICANDO AMOSTRAGEM PARA VALIDAÇÃO ========\n",
            "Aplicando amostragem de 5.0% aos dados...\n",
            "Empresas: 60,959,932 -> 3,050,055 registros\n",
            "Estabelecimentos: 64,017,368 -> 3,200,591 registros\n",
            "Sócios: 25,162,928 -> 1,259,042 registros\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limpeza dos dados"
      ],
      "metadata": {
        "id": "z_CMaJWWfJOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_empresas = df_empresas.na.fill({\n",
        "    'Porte_da_Empresa': 0,\n",
        "    'Razao_Social_Nome_Empresarial': 'Não Informado',\n",
        "    })"
      ],
      "metadata": {
        "id": "QXATuiysfNby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_estabelecimentos = df_estabelecimentos.na.fill({\n",
        "    'Nome_Fantasia': 'Não Informado',\n",
        "    'Tipo_de_Logradouro': 'Não Informado',\n",
        "    'Logradouro': 'Não Informado',\n",
        "    'Numero': 'SN',\n",
        "    'Complemento': 'Não Informado',\n",
        "    'Bairro': 'Não Informado',\n",
        "    'Cep': 'Não Informado',\n",
        "    'UF': 'Não Informado',\n",
        "    'ddd_1': 0,\n",
        "    'Telefone_1': 0,\n",
        "    'ddd_2': 0,\n",
        "    'Telefone_2': 0,\n",
        "    'ddd_do_fax': 0,\n",
        "    'Fax': 0,\n",
        "    'Correio_Eletronico': 'Não Informado'\n",
        "\n",
        "})"
      ],
      "metadata": {
        "id": "7nHR_FqzfPCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_socios = df_socios.na.fill({'Nome_do_Socio_ou_Razao_Social': 'Não Informado',\n",
        "                               'CNPJ_ou_CPF_do_Socio': 'Não Informado',\n",
        "                               'Nome_do_Representante': 'Não Informado'\n",
        "\n",
        "})"
      ],
      "metadata": {
        "id": "jLYKu3pxfTYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removendo duplicatas\n",
        "df_emrpesas = df_empresas.dropDuplicates()\n",
        "df_estabelecimentos = df_estabelecimentos.dropDuplicates()\n",
        "df_socios = df_socios.dropDuplicates()"
      ],
      "metadata": {
        "id": "XxkKk5RbfWHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformações iniciais nos dados carregados\n",
        "\n",
        "Esta etapa realiza:\n",
        "1. Conversão de formatos de data para o padrão dd-MM-yyyy;\n",
        "2. Transformação de valores monetários (como capital social) para o tipo adequado;\n",
        "3. Criação de classificações setoriais baseadas nos códigos CNAE;\n",
        "\n",
        "Estas transformações preparam os dados para análises posteriores e facilitam a integração das diferentes tabelas."
      ],
      "metadata": {
        "id": "pFCeDGkO-C50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============== TRANSFORMAÇÃO DE DADOS ===============\n",
        "print(\"\\n\\n======== TRANSFORMANDO DADOS ========\")\n",
        "\n",
        "# Estatísticas para debug\n",
        "print(f\"\\nRegistros de Empresas: {df_empresas.count():,}\")\n",
        "print(f\"Registros de Estabelecimentos: {df_estabelecimentos.count():,}\")\n",
        "print(f\"Registros de Socios: {df_socios.count():,}\")\n",
        "print(f\"Registros de Simples: {df_simples.count():,}\")\n",
        "\n",
        "print(\"\\nAplicando conversões de data...\")\n",
        "# Converter colunas de data para o tipo de dado correto\n",
        "df_simples = Time_Convert(df_simples, date_simples)\n",
        "df_estabelecimentos = Time_Convert(df_estabelecimentos, date_estabelecimentos)\n",
        "df_socios = Time_Convert(df_socios, date_socios)\n",
        "\n",
        "print(\"\\nConvertendo Capital Social para float...\")\n",
        "# Transformar a Coluna 'Capital_Social_da_Empresa' em FloatType\n",
        "df_empresas = df_empresas.withColumn(\n",
        "    'Capital_Social_da_Empresa',\n",
        "    regexp_replace(col('Capital_Social_da_Empresa'), \"\\\\.\", \"\")  # Remove pontos de milhar\n",
        ").withColumn(\n",
        "    \"Capital_Social_da_Empresa\",\n",
        "    regexp_replace(col('Capital_Social_da_Empresa'), \",\", \".\")  # Substitui vírgula por ponto\n",
        ").withColumn(\n",
        "    'Capital_Social_da_Empresa',\n",
        "    col('Capital_Social_da_Empresa').cast(FloatType())  # Converte para FloatType\n",
        ")\n"
      ],
      "metadata": {
        "id": "_dagRoxI9_Wo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44276d81-b42e-49a9-f95e-92da2a817b7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "======== TRANSFORMANDO DADOS ========\n",
            "\n",
            "Registros de Empresas: 3,050,055\n",
            "Registros de Estabelecimentos: 3,200,591\n",
            "Registros de Socios: 1,259,041\n",
            "Registros de Simples: 41,720,964\n",
            "\n",
            "Aplicando conversões de data...\n",
            "\n",
            "Convertendo Capital Social para float...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processo de enriquecimento e integração dos dados\n",
        "\n",
        "Esta etapa conecta as tabelas principais com as tabelas de referência para:\n",
        "1. Substituir códigos por suas descrições textuais (tornando os dados mais legíveis);\n",
        "2. Enriquecer os registros com informações adicionais;\n",
        "3. Transformar valores numéricos em categorias mais significativas.\n",
        "\n",
        "A estratégia implementada realiza os joins em etapas, priorizando as tabelas menores primeiro, seguido de limpeza de memória para evitar problemas de OOM."
      ],
      "metadata": {
        "id": "_qUeBZM5-ZH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============== JOINS E INTEGRAÇÃO DE DADOS ===============\n",
        "print(\"\\n\\n======== REALIZANDO JOINS ENTRE TABELAS ========\")\n",
        "\n",
        "# Step 1: Primeiro aplicar os joins nas tabelas de referência (menores)\n",
        "# Isso enriquece os dados antes de fazer joins maiores\n",
        "print(\"\\nAplicando joins com tabelas de referência...\")\n",
        "\n",
        "try:\n",
        "    # Joins para tabela de estabelecimentos\n",
        "    print(\"  - Enriquecendo estabelecimentos com dados de países...\")\n",
        "    df_estabelecimentos = Join_Dataframes(df_estabelecimentos, df_paises, 'Pais', 'ID_Pais')\n",
        "\n",
        "    print(\"  - Enriquecendo estabelecimentos com dados de municípios...\")\n",
        "    df_estabelecimentos = Join_Dataframes(df_estabelecimentos, df_municipios, 'Municipio', 'ID_Municipio')\n",
        "\n",
        "    print(\"  - Enriquecendo estabelecimentos com motivos de situação cadastral...\")\n",
        "    df_estabelecimentos = Join_Dataframes(df_estabelecimentos, df_motivos, 'Motivo_Situacao_Cadastral', 'ID_Motivo')\n",
        "\n",
        "    # Joins para tabela de empresas\n",
        "    print(\"  - Enriquecendo empresas com naturezas jurídicas...\")\n",
        "    df_empresas = Join_Dataframes(df_empresas, df_naturezas, 'Codigo_Natureza_Juridica', 'ID_Natureza')\n",
        "\n",
        "    print(\"  - Enriquecendo empresas com qualificações dos responsáveis...\")\n",
        "    df_empresas = Join_Dataframes(df_empresas, df_qualificacoes, 'Qualificacao_do_Responsavel', 'ID_Qualificacao')\n",
        "\n",
        "    # Joins para tabela de sócios\n",
        "    print(\"  - Enriquecendo sócios com qualificações...\")\n",
        "    df_socios = Join_Dataframes(df_socios, df_qualificacoes, 'Qualificacao_do_Socio', 'ID_Qualificacao')\n",
        "\n",
        "    print(\"  - Enriquecendo sócios com qualificações dos representantes legais...\")\n",
        "    df_socios = Join_Dataframes(df_socios, df_qualificacoes, 'Qualificacao_do_Representante_Legal', 'ID_Qualificacao')\n",
        "\n",
        "    print(\"  - Enriquecendo sócios com dados de países...\")\n",
        "    df_socios = Join_Dataframes(df_socios, df_paises, 'Pais_Socio', 'ID_Pais')\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ERRO durante joins com tabelas de referência: {str(e)}\")\n",
        "    # Continuar com os dados que temos, mesmo em caso de erro parcial\n",
        "\n",
        "# OTIMIZAÇÃO: Processar dados de CNAE secundário apenas se necessário\n",
        "if False:  # Desativado para reduzir complexidade e consumo de memória\n",
        "    try:\n",
        "        print(\"\\nProcessando CNAEs secundários...\")\n",
        "        df_estabelecimentos_exploded = df_estabelecimentos.withColumn(\n",
        "            \"Cnae_Fiscal_Secundaria\",\n",
        "            explode(split(\"Cnae_Fiscal_Secundaria\", \",\"))\n",
        "        )\n",
        "\n",
        "        # Realizar Join com CNAE secundário\n",
        "        df_joined = df_estabelecimentos_exploded.join(\n",
        "            df_cnae,\n",
        "            df_estabelecimentos_exploded[\"Cnae_Fiscal_Secundaria\"] == df_cnae[\"ID_Cnae\"],\n",
        "            \"left\"\n",
        "        )\n",
        "\n",
        "        # Reagrupar dados após o join\n",
        "        df_final = df_joined.groupBy(df_estabelecimentos_exploded.columns).agg(\n",
        "            collect_list(\"Cnae\").alias(\"Cnae_Fiscal_Secundaria\")\n",
        "        )\n",
        "\n",
        "        # Usar df_final em vez de df_estabelecimentos para joins seguintes\n",
        "        df_estabelecimentos = df_final\n",
        "    except Exception as e:\n",
        "        print(f\"AVISO: Processamento de CNAEs secundários falhou: {str(e)}\")\n",
        "        print(\"Continuando com os CNAEs principais apenas.\")\n",
        "\n",
        "# Step 3: Limpar memória e cache para maximizar recursos disponíveis\n",
        "spark.catalog.clearCache()\n",
        "gc.collect()\n",
        "log_memory_usage()"
      ],
      "metadata": {
        "id": "TQ0Jc_UW91Zw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8cc6117-bf91-40c2-968f-7044e7263146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "======== REALIZANDO JOINS ENTRE TABELAS ========\n",
            "\n",
            "Aplicando joins com tabelas de referência...\n",
            "  - Enriquecendo estabelecimentos com dados de países...\n",
            "  - Enriquecendo estabelecimentos com dados de municípios...\n",
            "  - Enriquecendo estabelecimentos com motivos de situação cadastral...\n",
            "  - Enriquecendo empresas com naturezas jurídicas...\n",
            "  - Enriquecendo empresas com qualificações dos responsáveis...\n",
            "  - Enriquecendo sócios com qualificações...\n",
            "  - Enriquecendo sócios com qualificações dos representantes legais...\n",
            "  - Enriquecendo sócios com dados de países...\n",
            "Memória: 78.1% usada (9.58 GB de 12.67 GB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processamento final, análise e persistência dos resultados\n",
        "\n",
        "Esta etapa executa:\n",
        "1. Seleção de colunas mais relevantes para reduzir volume de dados;\n",
        "2. Joins incrementais entre as tabelas principais;\n",
        "3. Criação do CNPJ completo formatado;\n",
        "4. Reparticionamento e salvamento otimizado dos dados processados;\n",
        "5. Visualização segura de uma amostra dos resultados;\n",
        "6. Análise exploratória básica com contagens e distribuições;\n",
        "\n",
        "O código inclui tratamento de erros e mecanismos de recuperação para garantir que dados parciais possam ser salvos mesmo em caso de falhas."
      ],
      "metadata": {
        "id": "dH5uavxB-x3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============== PROCESSAMENTO DO DATASET FINAL ===============\n",
        "print(\"\\n\\n======== PROCESSANDO DATASET FINAL ========\")\n",
        "\n",
        "\n",
        "# ABORDAGEM OTIMIZADA: Realizar joins incrementais e escolher colunas específicas\n",
        "\n",
        "# 1. Selecionar apenas colunas necessárias de cada tabela\n",
        "# Isso reduz significativamente o volume de dados sendo transferido\n",
        "\n",
        "if MODO_AMOSTRA:\n",
        "  cols_estabelecimentos = [\n",
        "        'CNPJ_Basico', 'CNPJ_Ordem', 'CNPJ_Dv', 'Identificador_Matriz_Filial',\n",
        "        'Situacao_Cadastral', 'UF', 'Municipio', 'Cnae_Fiscal_Principal'\n",
        "  ]\n",
        "  cols_empresas = ['CNPJ_Basico', 'Razao_Social_Nome_Empresarial', 'Porte_da_Empresa', 'Capital_Social_da_Empresa']\n",
        "  cols_simples = ['CNPJ_Basico', 'Opcao_pelo_Simples']\n",
        "  cols_socios = ['CNPJ_Basico', 'Identitificador_de_Socio', 'Faixa_Etaria']\n",
        "\n",
        "  df_estabelecimentos_reduced = df_estabelecimentos.select(cols_estabelecimentos)\n",
        "  df_empresas_reduced = df_empresas.select(cols_empresas)\n",
        "  df_simples_reduced = df_simples.select(cols_simples)\n",
        "  df_socios_reduced = df_socios.select(cols_socios)\n",
        "\n",
        "  df_estabelecimentos = df_estabelecimentos_reduced\n",
        "  df_empresas = df_empresas_reduced\n",
        "  df_simples = df_simples_reduced\n",
        "  df_socios = df_socios_reduced\n",
        "\n",
        "# 2. Realizar joins progressivos, salvando resultados intermediários\n",
        "print(\"\\nRealizando join entre estabelecimentos e empresas...\")\n",
        "df_estemp = Join_Dataframes_Duplicated(df_estabelecimentos, df_empresas, 'CNPJ_Basico')\n",
        "df_estemp = Join_Dataframes_Duplicated(df_estemp, df_socios, 'CNPJ_Basico')\n",
        "\n",
        "# Checkpoint para dados intermediários (salvar e recarregar para melhor gerenciamento de memória)\n",
        "checkpoint_path = os.path.join(pasta_parquet, 'checkpoint_estemp.parquet')\n",
        "df_estemp.write.mode('overwrite').option(\"compression\", \"snappy\").parquet(checkpoint_path)\n",
        "\n",
        "# Liberação de memória dos DataFrames originais que não são mais necessários\n",
        "df_estabelecimentos = None\n",
        "df_empresas = None\n",
        "df_socios = None\n",
        "gc.collect()\n",
        "\n",
        "# Recarregar do checkpoint\n",
        "df_estemp = spark.read.parquet(checkpoint_path)\n",
        "\n",
        "print(\"\\nRealizando join com dados do Simples Nacional...\")\n",
        "df_combined = Join_Dataframes_Duplicated(df_estemp, df_simples, 'CNPJ_Basico')\n",
        "\n",
        "# Liberar mais memória\n",
        "df_estemp = None\n",
        "df_simples = None\n",
        "gc.collect()\n",
        "\n",
        "# 3. Criar coluna CNPJ completo\n",
        "print(\"\\nCriando coluna de CNPJ completo...\")\n",
        "df_final = df_combined.withColumn(\n",
        "     \"CNPJ\",\n",
        "     concat(col(\"CNPJ_Basico\"), lit(\"/\"), col(\"CNPJ_Ordem\"), lit(\"-\"), col(\"CNPJ_Dv\"))\n",
        ")\n",
        "\n",
        "# 4. Remover colunas redundantes\n",
        "df_final = df_final.drop(\"CNPJ_Basico\", \"CNPJ_Ordem\", \"CNPJ_Dv\")\n",
        "\n",
        "\n",
        "# 5. Salvar resultado final em Parquet\n",
        "print(\"\\nSalvando DataFrame final em formato Parquet...\")\n",
        "output_path = os.path.join(pasta_parquet, 'cnpj_completo_otimizado.parquet')\n",
        "\n",
        "# Reparticionamento para otimizar a escrita\n",
        "df_final = df_final.repartition(50)\n",
        "\n",
        "# Salvar com compressão Snappy\n",
        "df_final.write.mode('overwrite').option(\"compression\", \"snappy\").parquet(output_path)\n",
        "\n",
        "print(f\"\\nDados salvos com sucesso em: {output_path}\")\n",
        "\n",
        "# Liberar recursos\n",
        "spark.stop()\n",
        "print(\"\\nSessão Spark encerrada. Processamento concluído.\")"
      ],
      "metadata": {
        "id": "SDMsmt0Q9sIM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6174c9de-b55f-42ed-e233-f8f4f9201fc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "======== PROCESSANDO DATASET FINAL ========\n",
            "\n",
            "Realizando join entre estabelecimentos e empresas...\n",
            "Tabela secundária muito grande para broadcast: 3050055 registros. Reparticionando.\n",
            "Tabela secundária muito grande para broadcast: 1259041 registros. Reparticionando.\n",
            "AVISO: Diferença no número de registros após join em CNPJ_Basico: antes=3200591, depois=3208500\n",
            "\n",
            "Realizando join com dados do Simples Nacional...\n",
            "Tabela secundária muito grande para broadcast: 41720964 registros. Reparticionando.\n",
            "\n",
            "Criando coluna de CNPJ completo...\n",
            "\n",
            "Salvando DataFrame final em formato Parquet...\n",
            "\n",
            "Dados salvos com sucesso em: /content/drive/MyDrive/arquivos_cnpj/2025-01/parquet/cnpj_completo_otimizado.parquet\n",
            "\n",
            "Sessão Spark encerrada. Processamento concluído.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recarregando os dados finais para visualização"
      ],
      "metadata": {
        "id": "N7FN9ic7phDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar uma sessão Spark com configurações otimizadas\n",
        "spark = SparkSession.builder \\\n",
        "    .appName('Análise de Dados CNPJ') \\\n",
        "    .config(\"spark.driver.memory\", \"16g\") \\\n",
        "    .config(\"spark.executor.memory\", \"8g\") \\\n",
        "    .config(\"spark.executor.cores\", \"4\") \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "    .config(\"spark.default.parallelism\", \"100\") \\\n",
        "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
        "    .config(\"spark.memory.storageFraction\", \"0.2\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .config(\"spark.kryoserializer.buffer.max\", \"1g\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.files.maxPartitionBytes\", \"134217728\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "WJs4TKyvpgcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final = spark.read.parquet(os.path.join(pasta_parquet, 'cnpj_completo_otimizado.parquet'))"
      ],
      "metadata": {
        "id": "17TaJ5ecppnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8PFEkEMqDm1",
        "outputId": "80939f6c-b35d-413b-abbf-57fd139ec69e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Identificador_Matriz_Filial: integer (nullable = true)\n",
            " |-- Situacao_Cadastral: integer (nullable = true)\n",
            " |-- UF: string (nullable = true)\n",
            " |-- Municipio: string (nullable = true)\n",
            " |-- Cnae_Fiscal_Principal: string (nullable = true)\n",
            " |-- Razao_Social_Nome_Empresarial: string (nullable = true)\n",
            " |-- Porte_da_Empresa: integer (nullable = true)\n",
            " |-- Capital_Social_da_Empresa: float (nullable = true)\n",
            " |-- Identitificador_de_Socio: integer (nullable = true)\n",
            " |-- Faixa_Etaria: integer (nullable = true)\n",
            " |-- Opcao_pelo_Simples: string (nullable = true)\n",
            " |-- CNPJ: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Transformar códigos para descrições em todas as tabelas\n",
        "print(\"\\nTransformando códigos para descrições...\")\n",
        "\n",
        "# Transformações para empresas\n",
        "df_final = df_final.withColumn(\n",
        "    'Porte_da_Empresa',\n",
        "    when(col('Porte_da_Empresa') == 0, 'Não Informado')\n",
        "    .when(col('Porte_da_Empresa') == 1, 'Micro Empresa')\n",
        "    .when(col('Porte_da_Empresa') == 3, 'Empresa de Pequeno Porte')\n",
        "    .when(col('Porte_da_Empresa') == 5, 'Demais')\n",
        "    .otherwise('Não Informado')\n",
        ")\n",
        "\n",
        "# Transformações para estabelecimentos\n",
        "df_final = df_final.withColumn(\n",
        "    'Identificador_Matriz_Filial',\n",
        "    when(col('Identificador_Matriz_Filial') == 1, 'Matriz')\n",
        "    .when(col('Identificador_Matriz_Filial') == 2, 'Filial')\n",
        "    .otherwise('Não Informado')\n",
        ")\n",
        "\n",
        "df_final = df_final.withColumn(\n",
        "    'Situacao_Cadastral',\n",
        "    when(col('Situacao_Cadastral') == 1, 'Nula')\n",
        "    .when(col('Situacao_Cadastral') == 2, 'Ativa')\n",
        "    .when(col('Situacao_Cadastral') == 3, 'Suspensa')\n",
        "    .when(col('Situacao_Cadastral') == 4, 'Inapta')\n",
        "    .when(col('Situacao_Cadastral') == 8, 'Baixada')\n",
        "    .otherwise('Não Informado')\n",
        ")\n",
        "\n",
        "# Transformações para simples\n",
        "df_final = df_final.withColumn(\n",
        "    'Opcao_pelo_Simples',\n",
        "    when(col('Opcao_pelo_Simples') == 'S', 'Sim')\n",
        "    .when(col('Opcao_pelo_Simples') == 'N', 'Não')\n",
        "    .when(col('Opcao_pelo_Simples') == 'EM BRANCO', 'Outros')\n",
        "    .otherwise('Não Informado')\n",
        ")\n",
        "\n",
        "# Transformações para sócios\n",
        "df_final = df_final.withColumn(\n",
        "    'Identitificador_de_Socio',\n",
        "    when(col('Identitificador_de_Socio') == 1, 'Pessoa Jurídica')\n",
        "    .when(col('Identitificador_de_Socio') == 2, 'Pessoa Física')\n",
        "    .when(col('Identitificador_de_Socio') == 3, 'Estrangeiro')\n",
        "    .otherwise('Não Informado')\n",
        ")\n",
        "\n",
        "df_final = df_final.withColumn(\n",
        "    'Faixa_Etaria',\n",
        "    when(col('Faixa_Etaria') == 1, '0 a 12 anos')\n",
        "    .when(col('Faixa_Etaria') == 2, '13 a 20 anos')\n",
        "    .when(col('Faixa_Etaria') == 3, '21 a 30 anos')\n",
        "    .when(col('Faixa_Etaria') == 4, '31 a 40 anos')\n",
        "    .when(col('Faixa_Etaria') == 5, '41 a 50 anos')\n",
        "    .when(col('Faixa_Etaria') == 6, '51 a 60 anos')\n",
        "    .when(col('Faixa_Etaria') == 7, '61 a 70 anos')\n",
        "    .when(col('Faixa_Etaria') == 8, '71 a 80 anos')\n",
        "    .when(col('Faixa_Etaria') == 9, '>80 anos')\n",
        "    .when(col('Faixa_Etaria') == 0, 'Não se aplica')\n",
        "    .otherwise('Falso')\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZW9j4QA5cd5",
        "outputId": "97a0bec7-22b9-498d-abb2-6320bd973738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Transformando códigos para descrições...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_final = df_final.fillna({\n",
        "    'Razao_Social_Nome_Empresarial': 'Não Informado',\n",
        "    'Capital_Social_da_Empresa': 0,\n",
        "})"
      ],
      "metadata": {
        "id": "JZce8zdhLbg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nCriando coluna de Ramo baseada no CNAE...\")\n",
        "# Criar coluna 'Ramo' baseada no CNAE usando o mapeamento\n",
        "mapping_expr = create_map([lit(x) for x in chain(*cnae_map.items())])\n",
        "\n",
        "df_final = df_final.withColumn(\n",
        "    'Ramo',\n",
        "    when(mapping_expr.getItem(substring(col('Cnae_Fiscal_Principal'), 1, 4)).isNotNull(),\n",
        "         mapping_expr.getItem(substring(col('Cnae_Fiscal_Principal'), 1, 4)))\n",
        "    .when(mapping_expr.getItem(substring(col('Cnae_Fiscal_Principal'), 1, 2)).isNotNull(),\n",
        "         mapping_expr.getItem(substring(col('Cnae_Fiscal_Principal'), 1, 2)))\n",
        "    .otherwise('Outros')\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npvmE9ld6vXH",
        "outputId": "736017a1-730d-4387-c474-ba66e8c75ebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Criando coluna de Ramo baseada no CNAE...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsYim7Qp7CqL",
        "outputId": "96f59cef-1390-41a7-feb5-08815fabe39d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------+------------------+---+----------------+---------------------+-----------------------------+----------------+-------------------------+------------------------+------------+------------------+----------------+--------------------+\n",
            "|Identificador_Matriz_Filial|Situacao_Cadastral| UF|       Municipio|Cnae_Fiscal_Principal|Razao_Social_Nome_Empresarial|Porte_da_Empresa|Capital_Social_da_Empresa|Identitificador_de_Socio|Faixa_Etaria|Opcao_pelo_Simples|            CNPJ|                Ramo|\n",
            "+---------------------------+------------------+---+----------------+---------------------+-----------------------------+----------------+-------------------------+------------------------+------------+------------------+----------------+--------------------+\n",
            "|                     Filial|           Baixada| BA|LAURO DE FREITAS|              4789099|                Não Informado|   Não informado|                      0.0|           Não Informado|       Falso|     Não Informado|15657828/0006-07|Conteúdo sensível...|\n",
            "|                     Matriz|           Baixada| SP|          IBIUNA|              4330403|                Não Informado|   Não informado|                      0.0|           Não Informado|       Falso|               Não|26367372/0001-44|              Outros|\n",
            "|                     Matriz|             Ativa| RS|        BROCHIER|              8712300|                Não Informado|   Não informado|                      0.0|           Não Informado|       Falso|               Sim|44113572/0001-52|              Outros|\n",
            "|                     Matriz|             Ativa| SP|       SAO PAULO|              8599699|                Não Informado|   Não informado|                      0.0|           Não Informado|       Falso|               Sim|41962381/0001-77|Consultoria e Tre...|\n",
            "|                     Matriz|           Baixada| SP|             JAU|              1531901|                Não Informado|   Não informado|                      0.0|           Não Informado|       Falso|     Não Informado|62964275/0001-00|              Outros|\n",
            "|                     Matriz|           Baixada| SP|       SAO PAULO|              9511800|                Não Informado|   Não informado|                      0.0|           Não Informado|       Falso|               Não|24416920/0001-53|              Outros|\n",
            "|                     Matriz|             Ativa| MA|     SANTA LUZIA|              9492800|                Não Informado|   Não informado|                      0.0|           Não Informado|       Falso|     Não Informado|15745404/0001-81|              Outros|\n",
            "|                     Matriz|            Inapta| SC|        IMBITUBA|              4771701|                Não Informado|   Não informado|                      0.0|           Não Informado|       Falso|               Não|10582200/0001-53|                Moda|\n",
            "|                     Matriz|             Ativa| SP|  EMBU DAS ARTES|              4712100|                Não Informado|   Não informado|                      0.0|           Não Informado|       Falso|               Sim|32459359/0001-18|   Comércio e varejo|\n",
            "|                     Matriz|             Ativa| SP|     SANTO ANDRE|              7311400|                Não Informado|   Não informado|                      0.0|           Não Informado|       Falso|     Não Informado|07432210/0001-06|Agências de Marke...|\n",
            "+---------------------------+------------------+---+----------------+---------------------+-----------------------------+----------------+-------------------------+------------------------+------------+------------------+----------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NOmjLcYMLmnt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}